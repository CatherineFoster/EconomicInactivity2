---
title: "Notebook 01: Extract PID and Econ"
author: 
 - "Jon Minton"
 - "Martin Taulbut"
format: 
  docx: 
    toc: true
    number-sections: true
  html:
    code-fold: true
    echo: true
editor: visual
execute: 
  echo: false
  warning: false
---

## Aim

The aim of this document is to extract the PID, year, and economic activity status from the UKHLS.\
Later iterations will also incorporate predictor variables.

## Preparation

First we load some packages, including the economic_inactivity package created as part of this project.

```{r}
library(tidyverse)
library(haven)
library(here)
#library(economic_inactivity)
```

Now we specify the base folder location containing all the UKHLS files we will want to access and iterate over.

```{r}
base_dir_location <- "big_data/UKDA-6614-stata/stata/stata13_se/ukhls"
```

Now we want to just get a list of the indresp files in the above directory

```{r}
indresp_files <- dir(here(base_dir_location), pattern = "[a-z]_indresp.dta", full.names = TRUE)
```

Now we want to declare the columns that we want to extract.

Initially this will just be economic activity status and PID. But soon we will expand this to cover additional explanatory variables too.

```{r}
variable_patterns <- c(
  "pidp",
  "^[a-z]{1}_jbstat"
)

```

Now to test we can select only the two (initially) columns of interest in the first data extract. Afterwards we will generalise to all data extracts

```{r}
data_wave_1 <- haven::read_dta(indresp_files[1])

```

Now to select only those variables which match the variable patterns above

```{r}
data_wave_1 %>% 
  select(pidp, ends_with("jbstat"))
```

We can now do this for all waves...

The steps are:

1.  Create a dataframe with the filenames to extract from
2.  Create a function that loads that dataframe and selects, and returns, only those columns we want to return
    1.  The above function would also split names like a_jbstat into the wave component and the variable name
3.  Bind the rows of all returns from the above

We will end with a very long dataframe with three columns: pidp, wave, and jbstat

```{r}
extract_vars_and_make_long <- function(dta, varname){
  out <- dta %>% 
     # hard-coded for now
     select(pidp, matches(paste0("^[a-z]{1}_", varname))) %>% 
     pivot_longer(-pidp) %>% 
    separate_wider_delim(
      name, 
      delim = "_", 
      names = c("wave", "variable")
    ) %>%
    mutate(value = as_factor(value, levels = 'labels') %>% 
             as.character()
    )
  out
}


```

Let's try the above function on a single loaded dataset

```{r}
long_dta <- tibble(
  file_loc = indresp_files
) %>% 
  mutate(
    all_data = map(file_loc, haven::read_dta)
  ) %>% 
  mutate(
    slimmed_data = map(all_data, extract_vars_and_make_long, varname = "jbstat")
  ) %>% 
  select(-all_data)

long_dta_combined <- bind_rows(long_dta$slimmed_data)

```

We now need to standardise the economic activity categories across all waves

We've created a spreadsheet with our proposed regroupings

```{r}
econ_act_groups <- readxl::read_excel(path = here("data/economic_activities_categories.xlsx"), sheet = 'categories') %>% 
  janitor::clean_names()

```

```{r}
econ_act_groupings_years <- 
  long_dta_combined %>% 
    # mutate(
    #   approximate_year = map_dbl(wave, function(x) match(x, letters) + 2008)
    # ) %>% 
    left_join(
      econ_act_groups,
      by  = c('value' = 'original')
    )
```

## Show above in wide format

I think we first want to confirm we can see the econ activity in wide format, with the pidp on the row and each column the econ act status in a different wave

```{r}
econ_act_groupings_years %>% select(pidp, wave, level_2_meso) %>% pivot_wider(names_from = 'wave', values_from = 'level_2_meso')
```

We are thinking about modelling approaches given we have panel data.

One way of starting would just be to predict whether inactive at time T given activity status at time T-1, and also given activity status at time T-2 (i.e. we can compared whether there's added value of 'remembering' the status of an individual's activity status two waves ago rather than just in the last wave.

Let's start to trial this approach by building two logistic regression models and comparing them.

We will initially just try to predict status in wave c.

```{r}
inact_in_wave_c_data <- econ_act_groupings_years %>% 
  select(pidp, wave, level_2_meso) %>%  
  pivot_wider(names_from = 'wave', values_from = 'level_2_meso') %>% 
  select(pidp, a:c) 

inact_in_wave_c_data <- inact_in_wave_c_data %>% 
  filter(complete.cases(.))


```

The above is an extract of the data for waves a, b and c. Only complete cases were retained so there are issues with representativeness/bias etc.

The three models below attempt to predict the probability of being economically inactive at wave c based on increasing amounts of information:

-   `mod_0`: No predictors (i.e. the general probability for all included in the sample frame of being inactive in wave c)

-   `mod_1`: Predicted on economic activity status in wave b only. (employed as reference category.

-   `mod_2`: Predicted on economic activity status in wave a and wave b. (i.e. two waves ago rather than just last wave)

```{r}
mod_0 <- glm(c == 'Inactive' ~ 1, data = inact_in_wave_c_data, family = "binomial") 
mod_1 <- glm(c == 'Inactive' ~ b, data = inact_in_wave_c_data, family = "binomial")
mod_2 <- glm(c == 'Inactive' ~ b + a, data = inact_in_wave_c_data, family = "binomial")


```

We can produce summaries of the models as follows:

```{r}
summary(mod_0)
```

```{r}
summary(mod_1)
```

```{r}
summary(mod_2)
```

Interpreting how the coefficients relate to predicted probabilities unfortunately involves some thinking. In the case of `mod_2`, for example, we need to note that the intercept value only relates to one particular condition/state, namely that in which the respondent was employed for the previous two waves. This is because Employed is used by the logistic regression as the reference category. We know Employed is the reference category is it is *not* listed as a coefficient for a or b.\
In order to convert from coefficients to predicted probabilities given previous respondent economic activity history we need to sum up those coefficients which are relevant to the condition we are trying to predict for, and pass this sum of coefficients into the x slot of logit (?) function which looks as follows:

$$
 \exp(x) / (1 + exp(x))
$$

For example, passing just the intercept into the above produces a value of around 0.04. This is the prediction for the case where a and b are both 'Employed' (the reference category.

And if we want to predict the case where the respondent was Employed at wave a and Unemployed at wave b, we should add the coefficient on bUnemployed to x. (i.e., approximately -3.14 + 1.49). This produces a predicted value of around 0.16.

Note the above calculations are approximate as the coefficients are truncated to 2dp.

We can get more exact estimates of the predicted probabilities either by using more decimal places, or by using the predict function, passing the background states we want to predict for as parameters in a dataframe passed in the newdata parameter in this function. For example:

```{r}
predict(mod_2, newdata = data.frame(a = 'Employed', b = 'Employed'), type = 'response')

predict(mod_2, newdata = data.frame(a = 'Employed', b = 'Unemployed'), type = 'response')
```

Note that type = 'response' is added as an argument to predict in order that the predict function returns the predicted probability rather than the log odds.

## Estimation of predicted transitions to inactivity based on above approach

The notebook \`predicted_transitions_to_inactivity_in_2011 and 2019.qmd\` formalises the logistic regression analysis above for both the start of the UKHLS and the last pre-pandemic year.

........

## Generalise to all years

Let's write a function that takes the following arguments:

-   `econ_group_col`: column of economic activity groupings

-   `pid_col`: name of column with pid

-   `t0`: letter or number indicating initial year (to calculate transitions *from*)

-   `time_col`: column indicating time variable (either a letter or a number)

```{r}
econ_act_groupings_years %>% 
  select(pidp, wave, level_2_meso) %>% 
  pivot_wider(names_from = 'wave', values_from = 'level_2_meso')

```

## Next steps

-   Automate to all years
-   Use more disaggregated categories
-   Work out better ways of presenting transitions (e.g. graphically)
-   Take other variables, link, and use to group (e.g. male/female; LLTI/no LLTI; age categories)
-   (Longer term): think about appropriate regression framework, e.g. multinomial regression
